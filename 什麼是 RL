RL 會去探索 (explore) 各種可能會讓它得到正向獎勵，或者它會再次運用 (exploit) 前一個可導致正向獎勵的動作

RL 所包含的元素:
1. 代理 (Agent):
一個能執行智能決策的軟體程式，就是 RL 中的學習者，Agent 藉由與環境的互動來作出某些動作，並根據所採取的動作來收到獎勵

2. 策略函數 (Policy):
定義了 Agent 在環境中的行為。他們代表了我們選擇某個動作來達到目標的方式。策略通常是用符號 π 來表示，可能是個查找表或更複雜的搜尋過程

3. 價值函數 (Value Function):
代表 Agent 在某個狀態中到底有多好。與策略相依，長用 v(s) 來表示。等於 Agent 從初始狀態開始之後所收到的總期望值獎勵。可以有多個; 因此最佳策略就是擁有最佳 Value Function 的那一個策略

4. 模型 (model):
是 Agent 在某個環境的表現。學習有兩種類型: 模型式 (model-based) 和 無模型 (model-free)。在 model-based 當中，Agent 會運用先前所學到的資訊來完成一項任務，在 model-free 中，Agent 只仰賴執行正確動作所得的試誤經驗。
舉例來說: 要找到一條從家裡到辦公室最快的路，在 model-based 當中，你就會走過往最快的路，而 model-free 則是嘗試所以的路徑後，選擇最快的一條。

RL 的環境類型
Agent 所互動的任何東西都稱為環境，環境就是 Agent 外部的世界，包含了其自身外的所有東西

1. 決定型環境 (Deterministic):
能根據當下的環境狀態來得知結果。舉例: 在西洋棋中，你移動一個棋子後，你知道下一個棋盤會長怎樣

2. 隨機型環境 (Stochastic):
無法從當下狀態來判斷結果，有非常高的不確定性。舉例: 你不知道擲骰子後它會出現什麼點數

3. 完全可觀察環境 (Fully observable):
不論何時都可以判斷系統狀態。舉例: 你在下棋中，你可以看到整個棋盤每個棋的位子，就可以選擇最佳走法了

4. 部分可觀察環境 (Partically observable):
無法隨時判斷系統狀態。舉例: 你在打撲克牌，你不知道對手的牌，你只能透過持續丟出來的牌去判斷對手的手牌狀態

5. 離散型環境 (discrete):
可由某狀態移動到另一個狀態的動作數量有限。舉例: 西洋棋的走法有限 (棋盤有限定格子數)

6. 連續型環境 (continuous):
某狀態移動到另一個狀態的動作數量為無限。舉例: 從出發點到目的地有各種不同的路徑

7. 世代型環境 (episodic):
也稱為非順序型 (non-sequential)，當下的動作不會影響到未來的動作

8. 非世代型環境 (non-episodic):
也稱為順序型 (sequential)，所有的動作都彼此相關
